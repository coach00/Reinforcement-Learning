import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt


#和Sarsa那个程序的环境一致，区别在于使用Q_learning来解决


WORLD_HEIGHT = 7
WORLD_WIDTH = 10

WIND = [0,0,0,1,1,1,2,2,1,0]

ACTION_UP = 0
ACTION_DOWN = 1
ACTION_LEFT = 2
ACTION_RIGHT = 3

#探索概率
EPSILON = 0.1

#步长/学习率
ALPHA = 0.5

#每一步的奖励
REWARD = -1

START = [3,0]
GOAL = [3,7]
ACTIONS = [ACTION_UP,ACTION_DOWN,ACTION_LEFT,ACTION_RIGHT]

def step(state,action):
    i,j = state
    if action == ACTION_UP:
        return [max(i - 1 - WIND[j],0),j]
    elif action == ACTION_DOWN:
        return [max(min(i+1-WIND[j],WORLD_HEIGHT-1),0),j]
    elif action == ACTION_LEFT:
        return [max(i - WIND[j],0),max(j-1,0)]
    elif action == ACTION_RIGHT:
        return [max(i - WIND[j],0),min(j+1,WORLD_WIDTH-1)]
    else:
        assert False

#play for an episode
def episode(q_value):
    time = 0
    state = START

    # if np.random.rand() < EPSILON:
    while state != GOAL:
        if np.random.binomial(1,EPSILON) == 1:
           action = np.random.choice(ACTIONS)
        else:
           values_ = q_value[state[0],state[1],:]
           action = np.random.choice([action_ for action_,value_ in enumerate(values_) if value_ == np.max(values_)])

        next_state = step(state,action)
        values_ = q_value[next_state[0],next_state[1],:]
        next_action = np.random.choice([action_ for action_, value_ in enumerate(values_) if value_ == np.max(values_)])

        #update
        q_value[state[0],state[1],action] += ALPHA * (REWARD + q_value[next_state[0],next_state[1],next_action] -
                                                      q_value[state[0],state[1],action])
        state = next_state
        time += 1
    return time

def sara():
    q_value = np.zeros((WORLD_HEIGHT,WORLD_WIDTH,4))
    episode_limit = 500

    steps = []
    ep = 0

    while ep < episode_limit:
        steps.append(episode(q_value))
        print('episode %d is completed' %ep)
        ep += 1


    plt.plot(np.arange(1,len(steps)+1),steps)
    plt.xlabel('episodes')
    plt.ylabel('steps')

    plt.savefig('./Q_learning2.png')
    plt.close()

    optimal_policy = []
    for i in range(0,WORLD_HEIGHT):
        optimal_policy.append([])
        for j in range(0,WORLD_WIDTH):
            if [i,j] == GOAL:
                optimal_policy[-1].append('G')
                continue
            bestAction = np.argmax(q_value[i,j,:])
            if bestAction == ACTION_UP:
                optimal_policy[-1].append('U')
            elif bestAction == ACTION_DOWN:
                optimal_policy[-1].append('D')
            elif bestAction == ACTION_LEFT:
                optimal_policy[-1].append('L')
            elif bestAction == ACTION_RIGHT:
                optimal_policy[-1].append('R')
    print('optimal policy is :')
    for row in optimal_policy:
        print(row)
    print('Wind strength for each column:\n{}'.format([str(w) for w in WIND]))
    #print('Q_table:')
    #print(q_value)

if __name__ == '__main__':
    sara()







